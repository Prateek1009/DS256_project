# -*- coding: utf-8 -*-
"""Copy of assigment1_sol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ow9KROJX-BxDgFX7EERithPDfJzd6219
"""

from google.colab import drive
drive.mount('/content/gdrive')

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#!wget -q https://mirrors.estointernet.in/apache/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz -P /content/drive/MyDrive # link wrong in blog
!tar xf /content/gdrive/Shareddrives/ds256-2022/spark-3.0.3-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop2.7"
import findspark
findspark.init()
findspark.find()
from pyspark.context import SparkContext
from pyspark.streaming import StreamingContext
from time import sleep
import pandas as pd
from tqdm import tqdm

# import time

# from pyspark import SparkContext
# from pyspark.streaming import StreamingContext

# if __name__ == "__main__":

#     sc = SparkContext(appName="PythonStreamingQueueStream")
#     ssc = StreamingContext(sc, 1)

#     # Create the queue through which RDDs can be pushed to
#     # a QueueInputDStream
#     rddQueue = []
#     for i in range(5):
#         rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]

#     # Create the QueueInputDStream and use it do some processing
#     inputStream = ssc.queueStream(rddQueue)
#     mappedStream = inputStream.map(lambda x: (x % 10, 1))
#     reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)
#     reducedStream.pprint()

#     ssc.start()
#     time.sleep(6)
#     ssc.stop(stopSparkContext=True, stopGraceFully=True)

import networkx as nx
import pickle
import itertools
def read_testcase(FOLDER):
    """
    Reads the GTFS network and preprocessed dict. If the dicts are not present, dict_builder_functions are called to construct them.
    Returns:
        stops_file (pandas.dataframe):  stops.txt file in GTFS.
        trips_file (pandas.dataframe): trips.txt file in GTFS.
        stop_times_file (pandas.dataframe): stop_times.txt file in GTFS.
        transfers_file (pandas.dataframe): dataframe with transfers (footpath) details.
        stops_dict (dict): keys: route_id, values: list of stop id in the route_id. Format-> dict[route_id] = [stop_id]
        stoptimes_dict (dict): keys: route ID, values: list of trips in the increasing order of start time. Format-> dict[route_ID] = [trip_1, trip_2] where trip_1 = [(stop id, arrival time), (stop id, arrival time)]
        footpath_dict (dict): keys: from stop_id, values: list of tuples of form (to stop id, footpath duration). Format-> dict[stop_id]=[(stop_id, footpath_duration)]
        route_by_stop_dict_new (dict): keys: stop_id, values: list of routes passing through the stop_id. Format-> dict[stop_id] = [route_id]
        idx_by_route_stop_dict (dict): preprocessed dict. Format {(route id, stop id): stop index in route}.
    """
    stops_file, trips_file, stop_times_file, transfers_file = load_all_db(FOLDER)
    stops_dict, stoptimes_dict, footpath_dict, routes_by_stop_dict, idx_by_route_stop_dict = load_all_dict(FOLDER)
    return stops_file, trips_file, stop_times_file, transfers_file, stops_dict, stoptimes_dict, footpath_dict, routes_by_stop_dict, idx_by_route_stop_dict

def load_all_dict(FOLDER):
    """
    Args:
        FOLDER (str): network folder.
    Returns:
        stops_dict (dict): preprocessed dict. Format {route_id: [ids of stops in the route]}.
        stoptimes_dict (dict): keys: route ID, values: list of trips in the increasing order of start time. Format-> dict[route_ID] = [trip_1, trip_2] where trip_1 = [(stop id, arrival time), (stop id, arrival time)]
        footpath_dict (dict): preprocessed dict. Format {from_stop_id: [(to_stop_id, footpath_time)]}.
        routes_by_stop_dict (dict): preprocessed dict. Format {stop_id: [id of routes passing through stop]}.
        idx_by_route_stop_dict (dict): preprocessed dict. Format {(route id, stop id): stop index in route}.
    """
    import pickle
    path = f"gdrive/MyDrive/dict_builder/{FOLDER}"
    with open(f'{path}/stops_dict_pkl.pkl', 'rb') as file:
        stops_dict = pickle.load(file)
    with open(f'{path}/stoptimes_dict_pkl.pkl', 'rb') as file:
        stoptimes_dict = pickle.load(file)
    with open(f'{path}/transfers_dict_full.pkl', 'rb') as file:
        footpath_dict = pickle.load(file)
    with open(f'{path}/routes_by_stop.pkl', 'rb') as file:
        routes_by_stop_dict = pickle.load(file)
    with open(f'{path}/idx_by_route_stop.pkl', 'rb') as file:
        idx_by_route_stop_dict = pickle.load(file)
    return stops_dict, stoptimes_dict, footpath_dict, routes_by_stop_dict, idx_by_route_stop_dict


def load_all_db(FOLDER):
    """
    Args:
        FOLDER (str): path to network folder.
    Returns:
        stops_file (pandas.dataframe): dataframe with stop details.
        trips_file (pandas.dataframe): dataframe with trip details.
        stop_times_file (pandas.dataframe): dataframe with stoptimes details.
        transfers_file (pandas.dataframe): dataframe with transfers (footpath) details.
    """
    import pandas as pd
    path = f"gdrive/MyDrive/GTFS/{FOLDER}"
    stops_file = pd.read_csv(f'{path}/stops.txt', sep=',').sort_values(by=['stop_id']).reset_index(drop=True)
    trips_file = pd.read_csv(f'{path}/trips.txt', sep=',')
    stop_times_file = pd.read_csv(f'{path}/stop_times.txt', sep=',')
    stop_times_file.arrival_time = pd.to_datetime(stop_times_file.arrival_time)
    if "route_id" not in stop_times_file.columns:
        stop_times_file = pd.merge(stop_times_file, trips_file, on='trip_id')
    transfers_file = pd.read_csv(f'{path}/transfers.txt', sep=',')
    return stops_file, trips_file, stop_times_file, transfers_file

def build_query_graph(SOURCE, FOLDER):
    with open(f"gdrive/MyDrive/transfer_pattern/{FOLDER}/{SOURCE}", "rb") as fp:   # Unpickling
        stored_transferpattern = pickle.load(fp)
    edge_list = [(edge[x], edge[x+1]) for edge in stored_transferpattern for x in range(len(edge)-1)]
    G = nx.DiGraph()
    G.add_edges_from(edge_list)
    adj_list = {int(x.split(',')[0]): [[int(x) for x in x.split(',')[1:]], [], []] for x in list(nx.generate_adjlist(G, delimiter=','))}
    return adj_list


def arrivaltme_query(stop1, stop2, deptime, routesindx_by_stop_dict, stoptimes_dict):
    routeidx1,routeidx2 = routesindx_by_stop_dict[stop1], routesindx_by_stop_dict[stop2]
    comon_routes = [(seq1, seq2) for seq1, seq2 in itertools.product(routeidx1,routeidx2) if seq1[0]==seq2[0] and seq1[1]<seq2[1]]
    arrival_times =[]
    for iternary in comon_routes:
        for trip_idx, trip in enumerate(stoptimes_dict[iternary[0][0]]):
            if trip[iternary[0][1]][1] >= deptime:
                arrival_times.append(trip[iternary[1][1]][1])
                break
    return min(arrival_times)

def multicriteria_dij(SOURCE, D_TIME, DESTINATION, footpath_dict, FOLDER, routesindx_by_stop_dict, stoptimes_dict):
    try:
        adjlist_dict = build_query_graph(SOURCE, FOLDER)
        t_l = []
        init_label = [D_TIME, 0, 0, 0, SOURCE]  #criteria1, criteria2, pred_node_id, idx_predece_label, self.node_id

        t_l.append(init_label)
        adjlist_dict[SOURCE][1].append(init_label)

        while t_l:
            l_q = min(t_l)
            q = l_q[4]

            #Move l_q from temporary to permanent
            t_l.remove(l_q)
            adjlist_dict[q][1].remove(l_q)
            adjlist_dict[q][2].append(l_q)

            h = adjlist_dict[q][2].index(l_q) #Store the position of label l_q from l_pq
            for j in adjlist_dict[q][0]:
                #Compute l_j the current label of vertex j
                try:
                    arr_time = arrivaltme_query(q, j, l_q[0], routesindx_by_stop_dict, stoptimes_dict)
                except ValueError:continue #No trip avaliable after l_q[0]
                l_j = list((arr_time, l_q[1] + 1, q, h, j))

                #Verify there is no label of j dominated by l_j
                dominated = False
                for label in adjlist_dict[j][1] + adjlist_dict[j][2]:
                    if label[0] <= l_j[0] and label[1] <= l_j[1]:
                        dominated = True
                        break

                if dominated == False:
                    #Store l_j as temporary label of j
                    adjlist_dict[j][1].append(l_j)
                    t_l.append(l_j)
                    #Delete all temporary labels of j dominated by l_j
                    for label in adjlist_dict[j][1]:
                        if l_j[0] == label[0] and l_j[1] == label[1]:
                            continue
                        if l_j[0] <= label[0] and l_j[1] <= label[1]:
                            adjlist_dict[j][1].remove(label)
                            t_l.remove(label)
            try:
                for j, footpath_time in footpath_dict[q]:
                    l_j = list((l_q[0]+ footpath_time, l_q[1], q, h, j))

                    #Verify there is no label of j dominated by l_j
                    dominated = False
                    for label in adjlist_dict[j][1] + adjlist_dict[j][2]:
                        if label[0] <= l_j[0] and label[1] <= l_j[1]:
                            dominated = True
                            break

                    if dominated == False:
                        #Store l_j as temporary label of j
                        adjlist_dict[j][1].append(l_j)
                        t_l.append(l_j)
                        #Delete all temporary labels of j dominated by l_j
                        for label in adjlist_dict[j][1]:
                            if l_j[0] == label[0] and l_j[1] == label[1]:
                                continue
                            if l_j[0] <= label[0] and l_j[1] <= label[1]:
                                adjlist_dict[j][1].remove(label)
                                t_l.remove(label)
            except KeyError: pass
        try:
            TP_output1 = f"Best arrival times are:{list(zip(*adjlist_dict[DESTINATION][2]))[0]}" 
            return TP_output1
        except KeyError:
            return "No Path exist"
    except FileNotFoundError:
      return "Invalid Source stop"

# from func_file4 import *
FOLDER = './sweden'
stops_file, trips_file, stop_times_file, transfers_file, stops_dict, stoptimes_dict, footpath_dict, routes_by_stop_dict, idx_by_route_stop_dict = read_testcase(FOLDER)
routesindx_by_stop_dict = {stop: list(zip(listofroutes, [stops_dict[x].index(stop) for x in listofroutes])) for stop, listofroutes in routes_by_stop_dict.items()}
OD_pairs = pd.read_csv(r"sweden_randomOD.csv", nrows=100)

sc = SparkContext(appName="PythonStreamingQueueStream")
ssc = StreamingContext(sc, 1)

OD_RDD = []
for _,i in tqdm(OD_pairs.iterrows()):
    OD_RDD.append(ssc.sparkContext.parallelize([[i.SOURCE, pd.to_datetime('2019-11-06 00:20:00'), i.DESTINATION]]))
input_stream = ssc.queueStream(OD_RDD)
ODmappedStream = input_stream.map(lambda x: multicriteria_dij(x[0], x[1], x[2], footpath_dict, FOLDER, routesindx_by_stop_dict, stoptimes_dict))
# ODmappedStream = input_stream.map(lambda x: x)
ODmappedStream.pprint()
ssc.start()
sleep(600)
ssc.stop(stopSparkContext=True, stopGraceFully=True)